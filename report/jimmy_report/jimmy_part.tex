\documentclass[8pts]{paper}
\usepackage{graphicx}
\usepackage[margin=0.8in]{geometry}
\usepackage{amsmath}
\input{jimmy_commands}
\usepackage[]{algorithm2e}
\usepackage{caption}
\begin{document}

\section{Statistical Methods}

To access the potential gain from intelligent data integration, we decided to compare the classification accuracy of kernel based integration technique with commonly used vector concatenation benchmarks. 

\subsection{Notation}
In describing different classification techniques, the following convention will be used:\\
\begin{itemize}
\setlength\itemsep{0em}
\item (x, y) pair denotes a feature vector x \mmin \rsup{m} and its corresponding target value $\in \{0, 1\}$ .\\
\item \xud{j} denotes the jth feature in x.\\
\item there are N training examples and ith training pair is represented by (\xsu{i}, \ysu{i} ).
\end{itemize}


\subsection{Vector Concatenation Benchmarks}

\subsubsection*{Gaussian Naive Bayes}
\underline{\textbf{Model:}}\\
The model assumes that conditioned on the value of y, every feature \xud{j} is generated independently i.e. $P(x|y) = \prod_{j=1}{m} P(x_j|y)$. In addition we assume that each $P(x_j|y) \sim \bold{N(\mu_{j,y}, \sigma_y)}$ . To make a prediction for an unseen data point x, we would only need to compute  its posterior probability $P(y|x)$ and pick the more likely case.\\

\underline{\textbf{Properties:}}\\
The formulation is clean and simple. It is extremely fast train and is quite resistant ti extra noise dimensions. 
The excessively strong assumption of independence means that we could only fit 2nd degree decision boundaries, so it is expected to perform poorly on our highly complex XOR dataset.\\

\underline{\textbf{Implementation:}}\\
We used sklearn's Gaussian Naive Bayes classifier. No hyper-parameter tuning is required. \\

\subsubsection*{Random Forest}
\underline{\textbf{Model:}}\\
We use a group of decision trees to make a prediction for an x. Each decision tree is based by a bootstrap sample of the training data and the splitting nodes are constrained to a random subset of all features. In this manner, the over-fit of decision tree could be controlled.\\

\underline{\textbf{Properties:}}\\
Random forest is known to be very resistant to extra noise dimensions since uninformative feature would never be used for a split in a decision tree. \\

\underline{\textbf{Implementation:}}\\
We used cross validation to select the best number of trees $\bar{t} \in \{10, 100\}$.\\

\subsubsection*{K-Nearest Neighbors}
\underline{\textbf{Model:}}\\
KNN makes a prediction on an unknown data-point by the majority voting of the K-closest points i.e. $$f(x) = sign(\sum_{x^i \in \text{K closest points}} y^i) $$ \\

\underline{\textbf{Properties:}}\\
KNN is able to fit extremely complex decision boundaries. However, it often suffers from the curse of high dimensionality i.e. when the training data only occupies a tiny fraction of the high dimensional feature space. Moreover, it could not discriminate against noise dimensions since they are incorporated as a part of Euclidean distance. \\

\underline{\textbf{Implementation:}}\\
We used cross validation to select the best number of neighbors $\bar{k} in \{1, 10\}$ \\

\subsubsection*{RBF Support Vector Machine}
\underline{\textbf{Model:}}\\
Similar to KNN, support vector machine makes recommendation by considering the weighted voting of a set of similar data points\\
$$f(x) = sign(\sum_{i=1}^{N} y^i d^i k(x, x^i)) \text{where K(x, $x^i$)} = exp(-\gamma|x - x^i|^2)$$
where $d^i$ is learned from data by finding the largest margin separating hyperplane in the projected space.\\

\underline{\textbf{Properties:}}\\
SVM can fit complex decision boundaries and is robust for high dimension data as compared to KNN. Those two reasons has made it the first go-to algorithm for classification tasks. However, it suffers from the same problem as KNN: it is unable to discriminate against extra noise dimensions.\\

\underline{\textbf{Implementation:}}\\
Cross-validation is performed to search for the best regularization parameter $\bar{C} \in \{0.1, 1, 10, 100\}$ and $\bar{\gamma} \in \{.01, .1, 1, 10\}$\\

\subsection{Generalized Multiple Kernel Learning}
Kernel methods allows the client to design custom kernels to incorporate prior knowledge about the dataset. We use multiple kernel framework to incorporate our knowledge that the data comes from two `independent' sources.\\

\underline{\textbf{Model:}}\\
The final trained model is almost the same as those from SVM. The only difference is that the kernel function is actually learned from data, as opposed to SVM's predefined kernel. More specifically, the final kernel is a convex combination of a set of predefined kernel functions: $$K(x, x^i) = \sum_{q=1}^{Q} \theta_q * K_q(x, x^i) \text{where }\theta_q \text{is learned from data}.$$

\underline{\textbf{Formulation as Optimization Problem}}
\begin{align*}
&\text{\minover{\theta, w, b}} & &C  \frac{1}{N} \sum_{i=1}^{N} L(f_{\theta, w, b}(x^i), y^i) + 1/2 |w|^2 \\
&\text{subject to} & &\alpha|\theta|_2^2 + (1-\alpha){|\theta|_1} \leq 1\\
\end{align*}
Where C is the regularization parameter, $\alpha$ is the elastic net parameter, L is the hinge loss function and $$f_{\theta, w, b}(x^i) = \sum_{q=1}^{Q} w_q \phi_q(x^i) \sqrt{\theta_q} + b $$
Now if we define v by $v_q := \frac{w_q}{\sqrt{\theta_q}}$, then we have a convex optimization problem:
\begin{align*}
&\text{\minover{\theta, v, b}} & &C  \frac{1}{N} \sum_{i=1}^{N} L(f_{\theta, w, b}(x^i), y^i) + 1/2 |w|^2 \\
&\text{subject to} & &\alpha|\theta|_2^2 + (1-\alpha){|\theta|_1} \leq 1\\
&\text{Where}  & &f_{\theta, v, b}(x^i) = \sum_{q=1}^{Q} v_q \phi_q(x^i)  + b
\end{align*}

Observe that the elastic net constrain ensures sparsity and grouping effect for the set of kernels chosen for the final model.\\

\underline{\textbf{Optimization Algorithm}}

\begin{algorithm}[H]
 \textbf{Parameter}:Suboptimality tolerance, $\epsilon$

 \textbf{Initialization:} Let t = 0 and $\theta$ be uniformly initilaized subject to the elastic net contraint\\
 \Repeat{difference between the unpper bound and lower bound is less than \eps}{
  1) Solve dual problem $\alpha^{t} = \underset{\alpha}{argmax}D(\theta^{t-1}, \alpha)$ \;
  2) Construct a cutting plane model, $h^t(\theta) = \underset{1 \leq i \leq t}{max} D(\theta, \alpha^i)$ \;
  3) Calculate a lower bound and an unpper bound for the optimal solution $\overline{D_t}, \underline{D_t}$ and an improvement set level set L = \{$\theta: h^t(\theta) \leq \text{some convex combination of }\overline{D_t}, \underline{D_t}$\}\;
  4) Project $\theta^{t-1}$ to L to obtain $\theta^t$
}
 \caption{Level method for the MKL[1]}
\end{algorithm}

\vspace{10 mm}
\underline{\textbf{Implementation}}\\
For each `independent' data source, we constructed 10 RBF kernels with $\gamma \in 2**\{-3, -2, -1 ..., 6\}$. Then we trained our model with C fixed to 100. \\

\begin{minipage}{\linewidth}
\begin{center}
\includegraphics[scale=.4]{implementation_flowchart}
\captionof{figure}{Flow Chart of Our Model}
\end{center}
\end{minipage}



\end{document}
