\documentclass{article}
\usepackage{courier}
\renewcommand{\ttdefault}{pcr}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}
\setlength{\parindent}{0cm}

\begin{document}
\title{Data Integration on High-Difficulty Binary Classification}
\author{Julia Finch, Jesse Hellemn, Kentaro Hoffman, Zhe Zhang}
\maketitle


\section{Overview and Motivation}
For many fields, especially in medicine and the social sciences, it is
increasingly common for data to come from multiple disparate data sources. For
example, a sociologist might be interested in predicting future income with two
datasets, family environment and school environment. For the best analysis, all
data sources should be utilized.


A naive approach is to concatenate the sources together into one large feature
matrix. Although simple and computationally efficient, this method throws away
information about the separate sources and forces downstream processing to
treat all of the sources in the same way. Various data integration techniques
have been proposed to more cleverly and effectively combine multiple sources.
Unfortunately, most of these techniques are poorly understood or poorly tested,
and there has been no systematic evaluation of their effectiveness.


This paper evaluates Generalized Metric Kernel Learning (GMKL) over simulated
data.


\section{Data Generation}

\subsection{Criteria of Good Simulated Data}

In order to understand the behavior of the data integration techniques, we
systematically created data that was:

\begin{centering}
\begin{enumerate}
    \item \label{criteria:separable} the two classes are separated by a true
        decision boundary that is known and calculable
    \begin{enumerate}
        \item or the two classes can be specified to overlap with percentage
            $p$, where $p=0$ leads to no overlap and $p=50$\% leads to complete
            overlap
    \end{enumerate}
    \item the decision boundary's complexity can be parametrized and controlled
    \item the reliability of each source can be specified exactly
    \item the noisyness of each source can be specified exactly
    \item \label{criteria:noise_dims} extra meaningless dimensions can be added
        to each source
\end{enumerate}
\end{centering}

Suppose that a classifier $C$ only obtains 60\% classification accuracy on a
dataset $D$ (with datapoints evenly split amongst 2 classes). This could be
attributable to either:
\begin{itemize}
    \item The classifier is not well suited to certain properties of dataset $D$
    \item 80\% of both classes overlap with each other. The best possible
        strategy in this area of overlap is to guess the class with 50\%
        accuracy. An optimal classifier will then guess 40\% of the datapoints
        correctly and also classify the 20\% of non-overlapping datapoints
        perfectly
\end{itemize}
Criterion \ref{criteria:separable} ensures that the latter case does not occur,
so that classifier performance is attributable solely to its efficacy on
certain types of data.




\subsection{Data Generation Models}

\subsubsection{Random Coefficient Linear Model}

A simple, common way to simulate data is to use a linear model with random
coefficients. This model generates data by:
\begin{enumerate}
    \item Specify the number of true latent variables $K$ along with the number
        of visible variables $N$
    \item Specify two distribution $D_j^0$ and $D_j^1$ of each latent variable
        $z_j$, $1 \leq j \leq K$, where $D_j^0$ is the distribution of $z_j$
        for negative classes and $D_j^1$ is the distribution of $z_j$ for
        positive classes
    \item Specify how each visible variable $x_j$ is generated from the latent
        variables $z_i$ with a formula of the form
        $$x_l = \sum_{i=1}^K \beta_i z_i + \sum_{i=1}^K\sum_{j=i}^K \beta_{i,j} z_i z_k + \text{higher-order-interactions}$$
        of linear combinations of arbitrary functions of the latent variables
    \item Specify the distribution of every $\beta$ in the above formula
    \item For every datapoint
    \begin{enumerate}
        \item Pick which class the datapoint belongs to
        \item Sample each $z_j$ from its respective distribution for this class
        \item Sample each $\beta$ from their respective distributions
        \item Generate each visible variable $x_j$ from its formula
    \end{enumerate}
\end{enumerate}

This model has significant shortcomings
\begin{itemize}
    \item It is not known how to systematically make the classification problem more or less difficult
    \item It is hard to know if the generated data overlaps
    \item It is hard to pick the $\beta$s to ensure that all of the criteria in \ref{list:data_critera} are satisfied
    \item There are many distributions and formulas to specify arbitrarily
\end{itemize}
It is common to simulate "realistic" data by randomly picking coefficients of a
linear model. This model has notable shortcomings, mainly that it is not
possible to know if the 2 classes of the generated data are 1) separated by a
complex decision boundary or 2) are overlapping and unseparable. Classification
techniques can only be accurately compared to each other on non-overlapping
data. If the two classes of generated data overlap and are not separable, then
there is no theoretical classifier that can correctly the overlapped part.

\subsubsection{Feed-forward Network Model}

We simulated data with a feed-forward fully connected conditional network
(figure \ref{fig:data_generation_model.png}).


Criteria \ref{criteria:noise_dims} is more difficult than it at first seems. In
order to create a source with $N_{useful}$ useful dimensions and $N_{noisy}$
meaningless dimensions, we needed to both 1) make $N_{noisy}$ dimensions of pure
noise and 2) make $N_{useful}$ dimensions, all of which are always useful. With a
random coefficient linear model, it is impossible to verify that all $N_{useful}$
dimensions are actually useful.


\section{Experiments}









\subsection{Experiemnt 2: Corrupted XOR Sources}

One variable that we decided to vary is the relative importance of the disparate sources. The purpose of this experiement is to evaluate how the various classifiers perform when one source is more important than the other. We want to see which classifers are able to identify the important sources/features.

In order to corrupt the sources, a Bernoulli experiemnt is performed for each source with a certain probability that the binomial indicator is flipped to 0 or 1 (whichever number it is currently not). A probability p_1 is assigned to the first source and a probability p_2 is assigned to the second source. These probabilities rnage from 0, meaning no corruption, to .5, meaning complete corruption. For example, say the true value of y is 0, p_1 is 0, and p_2 is 0.5. Then the data that stems from the first source accurately represents the value of y which is 0. The data that stems from the second source will accurately represent the value of y in 50 percent of the samples, but the other 50 percent of the samples it flips to represent 1. This means that all of the information is coming from the first source while the second source tells us nothing about the true value of y.

In order to test how the classifiers perform with different level of corruption on the Double XOR data, we varied the probability of corruption for each XOR source in increments of 0.1 from 0 to 0.5. We performed this experiment three times, holding the dimension of each source static at three, five, and seven. The results of this experiement performed on five dimensional XOR are displayed in the table below. See the appendix for the results from the three dimensional and seven dimensional experiements.

*insert table here*
*insert heatmaps here*

We see that the results of Experiment 2 are consistent with the results of Experiment 1 in that GMKL consistenetly outperforms concatenated GMKL which consistenly outperforms the standard classifiers. We also see that the accuracy of GMKL is the least affected by one of the two sources becoming corrupted. This is evidence that GMKL effectively identifies which sources are useful and relies primarily on those sources. 


\subsection{Experiement 3: Corrupted Sine and XOR Sources}

This experiment follows the same structure as Experiment 2 in terms of varying the corruption probabilities to each source. The difference in that instead of using two sources with the XOR structure, one XOR source was used and one Sine sources was used. We also varied the corruption probabilities in different increments. The corruption probability on each source ranged from 0 to 0.45 in increments of 0.15. We executed this experiment on three dimensional XOR and Sine data with a period of two. The results are displayed below.

*insert table here*
*insert heatmaps here*

Again we can see that GMKL outperforms Concatenated GMKL overall. From the heat maps above, we can observe that GMKL does not handle Sine data well. When the XOR source is heavily corrupted, the prediction accuracy of GMKL falls to barely better than a coin flip. On the opposite end of the heat map, we can note that GMKL performs very well when the Sine source is heavily corrupted as long as the XOR source remains intact. This means that GMKL is insensistive to unreliable data. GMKL is successfully able to identify the source that does not contribute to classification accuracy and ignore it. 



\subsection{Experiemnt 4: Noise Dimensions}

Another variable to explore is the number of noisy dimensions. This experiement explores how each classifier is able to handle additional dimensions that do not provide useful information regarding the class each point belongs to. Ideally, the classifiers are able to identify them as noise dimensions and ignores these dimension in their predictive model. 

For this experiment, we added the same number of noise dimensions to each XOR source. We performed this experiement three times, when there were three, five, and seven XOR dimensions. We varied the proportion of dimensions that were noise dimensions. The proportion varied from no noise dimensions, to a quarter, third, half, and then fianlly two thirds noise dimensions. The purpose of measuring the proportion of noise variables as opposed to the number of noise variables is so that the three experiements over different XOR dimensions can be accurately compared. 

The results from the three dimensional Double XOR experiment are represented in the plot below. The results from the five dimenional and seven dimensional XOR can be found in the appendix. 

*insert graph*

The purple line in the plot above shows how noise averse GMKL is while the accuracy of Concatenated GMKL falls when only a single noise dimension is added. When there are the same number of noise dimensions as there are useful dimensions, the accuracy of GMKL is still greater than 95 percent. This tells us that GMKL effectively ignores noise dimensions. It is worthy to note the Random Forest classifier is considered noise averse and is even less affected by noise than GMKL. However, GMKL is initially so much more accurate than Random Forest, that as far as we tested, the accuracy of GMKL remains superior to the Random Forest classifier. 































\end{document}

