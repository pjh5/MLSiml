\documentclass{article}
\usepackage{courier}
\renewcommand{\ttdefault}{pcr}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{enumerate}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}
\setlength{\parindent}{0cm}

\begin{document}
\title{Data Integration on High-Difficulty Binary Classification}
\author{Julia Finch, Jesse Hellemn, Kentaro Hoffman, Zhe Zhang}
\maketitle


\section{Overview and Motivation}
For many fields, especially in medicine and the social sciences, it is
increasingly common for data to come from multiple disparate data sources. For
example, a sociologist might be interested in predicting future income with two
datasets, family environment and school environment. For the best analysis, all
data sources should be utilized.


A naive approach is to concatenate the sources together into one large feature
matrix. Although simple and computationally efficient, this method throws away
information about the separate sources and forces downstream processing to
treat all of the sources in the same way. Various data integration techniques
have been proposed to more cleverly and effectively combine multiple sources.
Unfortunately, most of these techniques are poorly understood or poorly tested,
and there has been no systematic evaluation of their effectiveness.


This paper evaluates Generalized Metric Kernel Learning (GMKL) over simulated
data.


\section{Data Generation}

\subsection{Criteria of Good Simulated Data}

In order to understand the behavior of the data integration techniques, we
systematically created data that was:

\begin{centering}
\begin{enumerate}
    \item \label{criteria:separable} the two classes are separated by a true
        decision boundary that is known and calculable
    \begin{enumerate}
        \item or the two classes can be specified to overlap with percentage
            $p$, where $p=0$ leads to no overlap and $p=50$\% leads to complete
            overlap
    \end{enumerate}
    \item the decision boundary's complexity can be parametrized and controlled
    \item the reliability of each source can be specified exactly
    \item the noisyness of each source can be specified exactly
    \item \label{criteria:noise_dims} extra meaningless dimensions can be added
        to each source
\end{enumerate}
\end{centering}

Suppose that a classifier $C$ only obtains 60\% classification accuracy on a
dataset $D$ (with datapoints evenly split amongst 2 classes). This could be
attributable to either:
\begin{itemize}
    \item The classifier is not well suited to certain properties of dataset $D$
    \item 80\% of both classes overlap with each other. The best possible
        strategy in this area of overlap is to guess the class with 50\%
        accuracy. An optimal classifier will then guess 40\% of the datapoints
        correctly and also classify the 20\% of non-overlapping datapoints
        perfectly
\end{itemize}
Criterion \ref{criteria:separable} ensures that the latter case does not occur,
so that classifier performance is attributable solely to its efficacy on
certain types of data.




\subsection{Data Generation Models}

\subsubsection{Random Coefficient Linear Model}

A simple, common way to simulate data is to use a linear model with random
coefficients. This model generates data by:
\begin{enumerate}
    \item Specify the number of true latent variables $K$ along with the number
        of visible variables $N$
    \item Specify two distribution $D_j^0$ and $D_j^1$ of each latent variable
        $z_j$, $1 \leq j \leq K$, where $D_j^0$ is the distribution of $z_j$
        for negative classes and $D_j^1$ is the distribution of $z_j$ for
        positive classes
    \item Specify how each visible variable $x_j$ is generated from the latent
        variables $z_i$ with a formula of the form
        $$x_l = \sum_{i=1}^K \beta_i z_i + \sum_{i=1}^K\sum_{j=i}^K \beta_{i,j} z_i z_k + \text{higher-order-interactions}$$
        of linear combinations of arbitrary functions of the latent variables
    \item Specify the distribution of every $\beta$ in the above formula
    \item For every datapoint
    \begin{enumerate}
        \item Pick which class the datapoint belongs to
        \item Sample each $z_j$ from its respective distribution for this class
        \item Sample each $\beta$ from their respective distributions
        \item Generate each visible variable $x_j$ from its formula
    \end{enumerate}
\end{enumerate}

This model has significant shortcomings
\begin{itemize}
    \item It is not known how to systematically make the classification problem more or less difficult
    \item It is hard to know if the generated data overlaps
    \item It is hard to pick the $\beta$s to ensure that all of the criteria in \ref{list:data_critera} are satisfied
    \item There are many distributions and formulas to specify arbitrarily
\end{itemize}
It is common to simulate "realistic" data by randomly picking coefficients of a
linear model. This model has notable shortcomings, mainly that it is not
possible to know if the 2 classes of the generated data are 1) separated by a
complex decision boundary or 2) are overlapping and unseparable. Classification
techniques can only be accurately compared to each other on non-overlapping
data. If the two classes of generated data overlap and are not separable, then
there is no theoretical classifier that can correctly the overlapped part.

\subsubsection{Feed-forward Network Model}

We simulated data with a feed-forward fully connected conditional network
(figure \ref{fig:data_generation_model.png}).


Criteria \ref{criteria:noise_dims} is more difficult than it at first seems. In
order to create a source with $N_{useful}$ useful dimensions and $N_{noisy}$
meaningless dimensions, we needed to both 1) make $N_{noisy}$ dimensions of pure
noise and 2) make $N_{useful}$ dimensions, all of which are always useful. With a
random coefficient linear model, it is impossible to verify that all $N_{useful}$
dimensions are actually useful.


\section{Experiments}

\subsection{Experiment 1: Data Dimension Scaling}
With the prevalence of high dimensional data sets, we first would like to see if our data integration methods scale well as the dimension of the data sources increases. To this end, we generated several data sets of varying number of $N_{useful}$ dimensions from a double-XOR feed-forward network. The network is structured such that each data source receives the parity signal without corruption from the true source, y. Then it creates an arbitrary n-dimensional XOR of correct parity, which then becomes a 2n dimensional data vector.  A full set of paramters for this expierment can be seen in table 4.1. This network was chosen as it has no corruption of data sources, thus ensuring criteria 1. Second, as the dimensional of the XOR increases, the decision boundary becomes increasingly complicated. Once the data generation process is complete, classification was done by GMKL, Concatenate + GMKL, SVM, KNN, and Random Forest.
\begin{table}[h!]
\centering
 \begin{tabular}{|c c|} 
 \hline
Parameter & Value \\ 
\hline
Type of Data from each Data Source & [XOR, XOR]\\
$N_{useful}$ (For each data source) & [2,3,4,5,6,7] \\
$N_{noisey}$& 0\\
Number of Total Data points & 5000\\
Ratio of Training to Testing Data & 1:2\\
Probability of Data Source Signal Corruption & [0,0]\\
I must be missing some more parameters\\
\hline
 \end{tabular}
\end{table}
\textbf{These paramters values should be replaced with the correct variable names once section 2.2.2 is completed. }\\\
\textbf{Should the parameters for the classifiers be described here? or in an appendix? Or in the methods section? }
\begin{figure}[h]
\begin{center}
\includegraphics[scale=0.4]{experimentpic1.png}
\end{center}
\end{figure}


From figure \textbf{FIGURE NUMBER HERE}  we can see that as number of XOR dimensions increases, the classification accuracy of all of the classifiers decreases. This makes sense as the dimensionality of the data is increased without a corresponding increase in the number of data points, making the classifiers suffer from the curse of dimensionality. However, while all classifiers have decreasing accuracy, we can see that GMKL performs much better than the other classifiers with nearly a 35 percent better classification accuracy compared to the classical SVM at at 5 dimensions. In fact, there is a very intriguing pattern on display here as the the GMKL at 2n dimensions seems to be performing about as well as the classical SVM at n dimensions. This seems to indicate that maybe doing classification on the data sources as separate entities is highly desirabel for complicated classification problems. 

\begin{figure}
\begin{center}
\begin{tabular}{|c| c| c| c| c| }
\hline
GMKL Dimension & Accuracy & SVM Dimension & Accuracy & Difference\\
\hline
2 & a & 4 & a\\
\hline
3 & a & 6 &a
\end{tabular}
\end{center}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.6]{experimentpic2.png}
\includegraphics[scale=0.6]{experimentpic3.png}
\caption{test}
\end{figure}









\subsection{Experiemnt 2: Corrupted Sources}

One variable that we decided to vary is the relative importance of the disparate sources. The purpose of this experiement is to evaluate how the various classifiers perform when one source is more important than the other. We want to see which classifers are able to identify the important sources/features.

In order to corrupt the sources, a Bernoulli experiemnt is performed for each source with a certain probability that the binomial indicator is flipped to 0 or 1 (whichever number it is currently not). A probability $p_1$ is assigned to the first source and a probability$ p_2$ is assigned to the second source. These probabilities rnage from 0, meaning no corruption, to .5, meaning complete corruption. For example, say the true value of y is 0, $p_1$ is 0, and $p_2$ is 0.5. Then the data that stems from the first source accurately represents the value of y which is 0. The data that stems from the second source will accurately represent the value of y in 50 percent of the samples, but the other 50 percent of the samples it flips to represent 1. This means that all of the information is coming from the first source while the second source tells us nothing about the true value of y.

In order to test how the classifiers perform with different level of corruption on the Double XOR data, we varied the probability of corruption for each XOR source in increments of 0.1 from 0 to 0.5. We performed this experiment three times, holding the dimension of each source static at three, five, and seven. The results of this experiement performed on five dimensional XOR are displayed in the table below. See the appendix for the results from the three dimensional and seven dimensional experiements.

*insert table here *
*insert heatmaps here*

We see that the results of Experiment 2 are consistent with the results of Experiment 1 in that GMKL consistenetly outperforms concatenated GMKL which consistenly outperforms the standard classifiers. We also see that GMKL is the least affected by one of the two sources becoming corrupted. This is evidence that GMKL effectively identifies which sources are useful and relies primarily on those sources.


\section{Conclusion}
From these experiment we have seen not only the usefulness of GMKL, but also with the issues that traditional classifiers face in data integration. Higher dimensional data, corrupted data sources, noisy dimensions, and variably useful data sources have all been shown to negatively effect the classification accuracy of traditional classifiers such as KNN, SVM and random forest. Not only does this illustrate the usefulness of GMKL as tool for classification and data integration, but the way in which GMKL generates kernels separately seems to indicate the important of treating your data sources as separate entities and the damage you will do to your classification accuracy if you concatenate it all together without any thought. 


\section{Future Work}
For future work, first, we would like to see GMKL used on an real life data analysis problem too see if it shows the same utility that we saw previously. Second, since GMKL, and the creation of kernels is a fairly expensive operation to begin with, it would be nice to create a computationally less expensive version so that this could be run on larger datasets. Third, to show that GMKL is indeed up to the performance of other cutting edge algorithm, further comparison with other methods such as Neural Networks is necessary. And finally, if an investigation could be done of the theoretical properties of the GMKL and its relative performance compared to SVM would be appreciated to cement the useful GMKL as a data integration technique.































\end{document}

